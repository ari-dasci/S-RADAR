{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data/Beatriz/Doctorado GR/ADL_platform//S-ADL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 18:58:33.606253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-08 18:58:33.792810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757350713.885182  156612 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757350713.904358  156612 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-08 18:58:34.088859: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from SADL.time_series.algorithms import transformers\n",
    "from SADL.time_series.time_series_utils import TimeSeriesProcessor\n",
    "from SADL.visualization_module import DataVisualization\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'uci_id': 601, 'name': 'AI4I 2020 Predictive Maintenance Dataset', 'repository_url': 'https://archive.ics.uci.edu/dataset/601/ai4i+2020+predictive+maintenance+dataset', 'data_url': 'https://archive.ics.uci.edu/static/public/601/data.csv', 'abstract': 'The AI4I 2020 Predictive Maintenance Dataset is a synthetic dataset that reflects real predictive maintenance data encountered in industry.', 'area': 'Computer Science', 'tasks': ['Classification', 'Regression', 'Causal-Discovery'], 'characteristics': ['Multivariate', 'Time-Series'], 'num_instances': 10000, 'num_features': 6, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF'], 'index_col': ['UID', 'Product ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2020, 'last_updated': 'Wed Feb 14 2024', 'dataset_doi': '10.24432/C5HS5C', 'creators': [], 'intro_paper': {'ID': 386, 'type': 'NATIVE', 'title': 'Explainable Artificial Intelligence for Predictive Maintenance Applications', 'authors': 'S. Matzka', 'venue': 'International Conference on Artificial Intelligence for Industries', 'year': 2020, 'journal': None, 'DOI': '10.1109/AI4I49448.2020.00023', 'URL': 'https://www.semanticscholar.org/paper/b609c8e9ec6a2b8c642810953ef6dffe5766f7c1', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Since real predictive maintenance datasets are generally difficult to obtain and in particular difficult to publish, we present and provide a synthetic dataset that reflects real predictive maintenance encountered in industry to the best of our knowledge.\\r\\n\\r\\n\\r\\n\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': \"The dataset consists of 10 000 data points stored as rows with 14 features in columns\\r\\nUID: unique identifier ranging from 1 to 10000\\r\\nproduct ID: consisting of a letter L, M, or H for low (50% of all products), medium (30%) and high (20%) as product quality variants and a variant-specific serial number\\r\\nair temperature [K]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K\\r\\nprocess temperature [K]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\\r\\nrotational speed [rpm]: calculated from a power of 2860 W, overlaid with a normally distributed noise\\r\\ntorque [Nm]: torque values are normally distributed around 40 Nm with a Ïƒ = 10 Nm and no negative values. \\r\\ntool wear [min]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process. and a\\r\\n'machine failure' label that indicates, whether the machine has failed in this particular datapoint for any of the following failure modes are true.\\r\\n\\r\\nThe machine failure consists of five independent failure modes\\r\\ntool wear failure (TWF): the tool will be replaced of fail at a randomly selected tool wear time between 200 â€“ 240 mins (120 times in our dataset). At this point in time, the tool is replaced 69 times, and fails 51 times (randomly assigned).\\r\\nheat dissipation failure (HDF): heat dissipation causes a process failure, if the difference between air- and process temperature is below 8.6 K and the toolâ€™s rotational speed is below 1380 rpm. This is the case for 115 data points.\\r\\npower failure (PWF): the product of torque and rotational speed (in rad/s) equals the power required for the process. If this power is below 3500 W or above 9000 W, the process fails, which is the case 95 times in our dataset.\\r\\noverstrain failure (OSF): if the product of tool wear and torque exceeds 11,000 minNm for the L product variant (12,000 M, 13,000 H), the process fails due to overstrain. This is true for 98 datapoints.\\r\\nrandom failures (RNF): each process has a chance of 0,1 % to fail regardless of its process parameters. This is the case for only 5 datapoints, less than could be expected for 10,000 datapoints in our dataset.\\r\\n\\r\\nIf at least one of the above failure modes is true, the process fails and the 'machine failure' label is set to 1. It is therefore not transparent to the machine learning method, which of the failure modes has caused the process to fail \", 'citation': None}}\n",
      "Variable information:                    name     role         type demographic description units  \\\n",
      "0                   UID       ID      Integer        None        None  None   \n",
      "1            Product ID       ID  Categorical        None        None  None   \n",
      "2                  Type  Feature  Categorical        None        None  None   \n",
      "3       Air temperature  Feature   Continuous        None        None     K   \n",
      "4   Process temperature  Feature   Continuous        None        None     K   \n",
      "5      Rotational speed  Feature      Integer        None        None   rpm   \n",
      "6                Torque  Feature   Continuous        None        None    Nm   \n",
      "7             Tool wear  Feature      Integer        None        None   min   \n",
      "8       Machine failure   Target      Integer        None        None  None   \n",
      "9                   TWF   Target      Integer        None        None  None   \n",
      "10                  HDF   Target      Integer        None        None  None   \n",
      "11                  PWF   Target      Integer        None        None  None   \n",
      "12                  OSF   Target      Integer        None        None  None   \n",
      "13                  RNF   Target      Integer        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n"
     ]
    }
   ],
   "source": [
    "from SADL.time_series.time_series_datasets_uci import global_load as load_time_series \n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = load_time_series('ai4i_2020_predictive_maintenance_dataset')   #name dataset in static datasets uci repo\n",
    "labels = y[\"Machine failure\"]\n",
    "X = X.drop('Type',axis=1)  # remove Type or apply one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 5) (2000, 5) (8000,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "from SADL.time_series.preprocessing.preprocessing_ts import StandardScalerPreprocessing \n",
    "scaler = StandardScalerPreprocessing()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, labels, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Model Vanilla Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Params: {'label_parser': None, 'train_epochs': 5, 'batch_size': 16, 'lr': 0.001} \n",
      "Model Params: {'algorithm_': 'Transformer', 'size_enc_in': 5, 'size_dec_in': 5, 'ulayers_feedfwd': 128, 'seq_len': 24, 'd_qk': 64, 'd_v': 64, 'd_model': 64, 'n_layers': 2, 'n_heads': 8, 'dropout_rate': 0.1, 'attns_outs': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 16\n",
    "seq_len = 24\n",
    "input_dim = 5  # tanto encoder como decoder\n",
    "d_model = 64\n",
    "train_epochs = 10\n",
    "\n",
    "# Simulamos series de entrada\n",
    "X = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Instanciamos el modelo Transformer\n",
    "kwargs ={\n",
    "    \"algorithm_\": \"transformer\",\n",
    "    \"label_parser\": None,\n",
    "    \"size_enc_in\":input_dim,\n",
    "    \"size_dec_in\": input_dim,\n",
    "    \"ulayers_feedfwd\": 128,\n",
    "    \"seq_len\":seq_len,\n",
    "    \"d_qk\":64,\n",
    "    \"d_v\":64,\n",
    "    \"d_model\":d_model,\n",
    "    \"n_layers\": 2,\n",
    "    \"n_heads\": 8,\n",
    "   \"dropout_rate\": 0.1,\n",
    "   \"attns_outs\":False, \n",
    "    \"train_epochs\": train_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"lr\": 1e-3\n",
    "    }\n",
    "    \n",
    "\n",
    "model = transformers.TransformersAnomalyDetection(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7977, 24, 5)\n",
      "y_train shape: (7977, 24)\n",
      "X_test shape: (1977, 24, 5)\n",
      "y_test shape: (1977, 24)\n"
     ]
    }
   ],
   "source": [
    "processor = TimeSeriesProcessor(window_size= seq_len, step_size=1, future_prediction=False)\n",
    "X_train_windows, y_train_windows, X_test_windows, y_test_windows = processor.process_train_test(X_train, y_train, X_test, y_test)\n",
    "print(\"X_train shape:\", X_train_windows.shape)\n",
    "print(\"y_train shape:\", y_train_windows.shape)\n",
    "print(\"X_test shape:\", X_test_windows.shape)\n",
    "print(\"y_test shape:\", y_test_windows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Beatriz/Doctorado GR/ADL_platform/S-ADL/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([16, 24, 5])) that is different to the input size (torch.Size([24, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/data/Beatriz/Doctorado GR/ADL_platform/S-ADL/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([9, 24, 5])) that is different to the input size (torch.Size([24, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.004393\n",
      "Epoch 2/5, Loss: 0.980808\n",
      "Epoch 3/5, Loss: 0.979639\n",
      "Epoch 4/5, Loss: 0.979225\n",
      "Epoch 5/5, Loss: 0.973495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.transformers.TransformersAnomalyDetection at 0x7f6f1efc45b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.decision_function(X_train_windows)\n",
    "labels = model.predict(X_train_windows)\n",
    "\n",
    "print(\"Anomaly scores:\", scores.shape)\n",
    "print(\"Predicted labels:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_windows,y_test_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Model Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Params: {'label_parser': None, 'train_epochs': 5, 'batch_size': 16, 'lr': 0.001} \n",
      "Model Params: {'algorithm_': 'Informer', 'enc_in': 5, 'dec_in': 5, 'c_out': 5, 'seq_len': 24, 'label_len': 24, 'out_len': 24, 'factor': 5, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 128, 'dropout': 0.1, 'attn': 'prob', 'activation': 'gelu', 'output_attention': False, 'distil': True, 'mix': True}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len = 24\n",
    "pred_len = 24\n",
    "label_len = 24\n",
    "input_dim = 5  # tanto encoder como decoder\n",
    "d_model = 64\n",
    "train_epochs = 5\n",
    "\n",
    "# Simulamos series de entrada\n",
    "\n",
    "# Instanciamos el modelo Transformer\n",
    "kwargs = {\n",
    "    \"algorithm_\": \"informer\",\n",
    "    \"label_parser\": None,\n",
    "    \"enc_in\": input_dim,             # Number of input variables for the encoder\n",
    "    \"dec_in\": input_dim,             # Number of input variables for the decoder\n",
    "    \"c_out\": input_dim,              # Output dimension (change if needed)\n",
    "    \"seq_len\": seq_len,              # Input sequence length\n",
    "    \"label_len\": label_len,          # Length of the decoder input (label segment)\n",
    "    \"out_len\": pred_len,             # Prediction length\n",
    "    \"factor\": 5,                     # ProbSparse factor (used in ProbAttention)\n",
    "    \"d_model\": d_model,              # Model dimension\n",
    "    \"n_heads\": 8,                    # Number of attention heads\n",
    "    \"e_layers\": 2,                   # Number of encoder layers\n",
    "    \"d_layers\": 1,                   # Number of decoder layers\n",
    "    \"d_ff\": 128,                     # Feedforward network dimension\n",
    "    \"dropout\": 0.1,                  # Dropout rate\n",
    "    \"attn\": \"prob\",                  # Attention type: 'prob' or 'full'\n",
    "    \"activation\": \"gelu\",           # Activation function\n",
    "    \"output_attention\": False,       # Whether to output attention weights\n",
    "    \"distil\": True,                  # Whether to use distillation in the encoder\n",
    "    \"mix\": True,                     # Whether to use mixed attention in the decoder\n",
    "    \"train_epochs\": train_epochs,    # Number of training epochs\n",
    "    \"batch_size\": batch_size,        # Batch size\n",
    "    \"lr\": 1e-3                       # Learning rate\n",
    "}\n",
    "    \n",
    "\n",
    "model = transformers.TransformersAnomalyDetection(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Serie Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7977, 24, 5)\n",
      "y_train shape: (7977, 24)\n",
      "X_test shape: (1977, 24, 5)\n",
      "y_test shape: (1977, 24)\n"
     ]
    }
   ],
   "source": [
    "processor = TimeSeriesProcessor(window_size= seq_len, step_size=1, future_prediction=False)\n",
    "X_train_windows, y_train_windows, X_test_windows, y_test_windows = processor.process_train_test(X_train, y_train, X_test, y_test)\n",
    "print(\"X_train shape:\", X_train_windows.shape)\n",
    "print(\"y_train shape:\", y_train_windows.shape)\n",
    "print(\"X_test shape:\", X_test_windows.shape)\n",
    "print(\"y_test shape:\", y_test_windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Beatriz/Doctorado GR/ADL_platform/S-ADL/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([16, 24, 5])) that is different to the input size (torch.Size([24, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/data/Beatriz/Doctorado GR/ADL_platform/S-ADL/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([9, 24, 5])) that is different to the input size (torch.Size([24, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.018134\n",
      "Epoch 2/5, Loss: 0.987470\n",
      "Epoch 3/5, Loss: 0.974741\n",
      "Epoch 4/5, Loss: 0.964567\n",
      "Epoch 5/5, Loss: 0.959280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.transformers.TransformersAnomalyDetection at 0x7343ceeb9ea0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly scores: torch.Size([1977])\n",
      "Predicted labels: torch.Size([47448, 5])\n"
     ]
    }
   ],
   "source": [
    "scores = model.decision_function(X_test_windows)\n",
    "pred = model.predict(X_test_windows)\n",
    "\n",
    "print(\"Anomaly scores:\", scores.shape)\n",
    "print(\"Predicted labels:\", pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_windows,y_test_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = model.labels_preds\n",
    "labels_true = np.array(y_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = DataVisualization(data=X_test, plot_technique='plot_anomaly', dim_reduction_technique='PCA', y_true=labels_true, y_pred=labels_pred,subset_size_percent=0.2)\n",
    "visualizer.fit()\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Model Autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Params: {'label_parser': None, 'train_epochs': 5, 'batch_size': 16, 'lr': 0.001} \n",
      "Model Params: {'algorithm_': 'autoformer', 'seq_len': 24, 'label_len': 24, 'pred_len': 24, 'enc_in': 5, 'dec_in': 5, 'c_out': 5, 'd_model': 64, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 128, 'moving_avg': 25, 'factor': 5, 'dropout': 0.1, 'activation': 'gelu', 'output_attention': False}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len = 24\n",
    "pred_len = 24\n",
    "label_len = 24\n",
    "input_dim = 5       \n",
    "d_model = 64\n",
    "train_epochs = 5\n",
    "\n",
    "kwargs = {\n",
    "    \"algorithm_\": \"autoformer\",\n",
    "    \"label_parser\": None,\n",
    "    \"seq_len\": seq_len,\n",
    "    \"label_len\": label_len,\n",
    "    \"pred_len\": pred_len,\n",
    "    \"enc_in\": input_dim,\n",
    "    \"dec_in\": input_dim,\n",
    "    \"c_out\": input_dim,\n",
    "    \"d_model\": d_model,\n",
    "    \"n_heads\": 8,\n",
    "    \"e_layers\": 2,\n",
    "    \"d_layers\": 1,\n",
    "    \"d_ff\": 128,\n",
    "    \"moving_avg\": 25,\n",
    "    \"factor\": 5,\n",
    "    \"dropout\": 0.1,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"output_attention\": False,\n",
    "    \"train_epochs\": train_epochs,    # Number of training epochs\n",
    "    \"batch_size\": batch_size,        # Batch size\n",
    "    \"lr\": 1e-3                       # Learning rate\n",
    "}\n",
    "\n",
    "modelAutoformer = transformers.TransformersAnomalyDetection(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7977, 24, 5)\n",
      "y_train shape: (7977, 24)\n",
      "X_test shape: (1977, 24, 5)\n",
      "y_test shape: (1977, 24)\n"
     ]
    }
   ],
   "source": [
    "processor = TimeSeriesProcessor(window_size= seq_len, step_size=1, future_prediction=False)\n",
    "X_train_windows, y_train_windows, X_test_windows, y_test_windows = processor.process_train_test(X_train, y_train, X_test, y_test)\n",
    "print(\"X_train shape:\", X_train_windows.shape)\n",
    "print(\"y_train shape:\", y_train_windows.shape)\n",
    "print(\"X_test shape:\", X_test_windows.shape)\n",
    "print(\"y_test shape:\", y_test_windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Beatriz/Doctorado GR/ADL_platform/S-ADL/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([16, 24, 5])) that is different to the input size (torch.Size([24, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/data/Beatriz/Doctorado GR/ADL_platform/S-ADL/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([9, 24, 5])) that is different to the input size (torch.Size([24, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.014112\n",
      "Epoch 2/5, Loss: 0.992302\n",
      "Epoch 3/5, Loss: 0.989583\n",
      "Epoch 4/5, Loss: 0.987402\n",
      "Epoch 5/5, Loss: 0.986582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.transformers.TransformersAnomalyDetection at 0x78dd40dc6770>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAutoformer.fit(X_train_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly scores: torch.Size([1977])\n",
      "Predicted labels: torch.Size([47448, 5])\n"
     ]
    }
   ],
   "source": [
    "scores = modelAutoformer.decision_function(X_test_windows)\n",
    "pred = modelAutoformer.predict(X_test_windows)\n",
    "\n",
    "print(\"Anomaly scores:\", scores.shape)\n",
    "print(\"Predicted labels:\", pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelAutoformer.evaluate(X_test_windows,y_test_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = modelAutoformer.labels_preds\n",
    "labels_true = np.array(y_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = DataVisualization(data=X_test, plot_technique='plot_anomaly', dim_reduction_technique='PCA', y_true=labels_true, y_pred=labels_pred,subset_size_percent=0.2)\n",
    "visualizer.fit()\n",
    "visualizer.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
