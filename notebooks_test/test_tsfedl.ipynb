{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: KDDCup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 08:34:23.275303: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-29 08:34:23.275484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-29 08:34:23.489223: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-29 08:34:23.950015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-29 08:34:26.010020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.time_series_utils import TimeseriesDataset\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "data, attack_types, classes = dataset.readKDDCup99Dataset()\n",
    "data = sklearn.preprocessing.StandardScaler().fit_transform(data)\n",
    "\n",
    "attack_types_dict = {}\n",
    "cont=0\n",
    "for att in attack_types:\n",
    "    attack_types_dict[att]=cont\n",
    "    cont+=1\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    classes[i] = attack_types_dict[classes[i]]\n",
    "\n",
    "classes[classes!=attack_types_dict[\"normal\"]] = 1\n",
    "classes[classes==attack_types_dict[\"normal\"]] = 0\n",
    "classes_test = classes[-500000:]\n",
    "\n",
    "normal_train = np.where(classes[:-500000]==0)[0][:10000]\n",
    "train_dataset = TimeseriesDataset(torch.from_numpy(data[normal_train]).double(), seq_len=4)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 3, shuffle = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpkl\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     pkl\u001b[38;5;241m.\u001b[39mdump(\u001b[43mdata\u001b[49m[normal_train], f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "with open('test.pkl','wb') as f:\n",
    "    pkl.dump(data[normal_train], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load KDDCup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpkl\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "with open('test.pkl','rb') as f:\n",
    "    train_data = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 126)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL OhShuLi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'OhShuLih', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "OhShuLih(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=20, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 3, kernel_size=(20,), stride=(1,), padding=(19,), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(3, 6, kernel_size=(10,), stride=(1,), padding=(9,), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(6, 6, kernel_size=(5,), stride=(1,), padding=(4,), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(6, 20, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "train_dataset_fuera = TimeSeriesDatasetV2(torch.from_numpy(train_data).double(), window_size=4, forecast_size=1,permute_size = (1,0))\n",
    "train_loader_fuera = torch.utils.data.DataLoader(train_dataset_fuera, batch_size = 32, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "kwargs = {\"algorithm_\": \"ohshulih\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=20, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model1 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model1.model = model1.model.double()\n",
    "\n",
    "#print(model1.get_params())\n",
    "print(model1.model)\n",
    "\n",
    "#model1.fit(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 7.5 K \n",
      "2 | convolutions | Sequential | 7.9 K \n",
      "3 | lstm         | LSTM       | 2.2 K \n",
      "--------------------------------------------\n",
      "17.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.6 K    Total params\n",
      "0.071     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4baa6f2ddab54133b26ceef2f4113647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                           | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([12, 1, 126])) that is different to the input size (torch.Size([12, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f86e89ddb90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_loader_fuera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (27, 126)\n",
      "y_pred shape:(27, 126)\n",
      "Y Shape: (6, 126)\n",
      "y_pred shape:(6, 126)\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "scores = model1.decision_function(train_loader_fuera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9996\n",
      "10017\n"
     ]
    }
   ],
   "source": [
    "print(len(scores))\n",
    "print(27 *train_loader_fuera.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: LiOhShu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 7.0 K \n",
      "2 | convolutions | Sequential | 8.5 K \n",
      "3 | lstm         | LSTM       | 720   \n",
      "--------------------------------------------\n",
      "16.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.2 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'LihOhShu', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "LihOhShu(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=10, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 3, kernel_size=(20,), stride=(1,), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(3, 6, kernel_size=(10,), stride=(1,), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(6, 6, kernel_size=(5,), stride=(1,), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv1d(6, 6, kernel_size=(5,), stride=(1,), bias=False)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv1d(6, 6, kernel_size=(10,), stride=(1,), bias=False)\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(6, 10, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ae65b366b245da98d39f9b1b40af5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                           | 0/? [0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSFEDLerror fit():  Given groups=1, weight of size [3, 126, 20], expected input[1, 400, 126] to have 126 channels, but got 400 channels instead\n",
      "For further reference please see: https://s-tsfe-dl.readthedocs.io/en/latest/index.html\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [3, 126, 20], expected input[1, 400, 126] to have 126 channels, but got 400 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m model2\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(model2\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/S-ADL/SADL/time_series/algorithms/tsfedl.py:87\u001b[0m, in \u001b[0;36mTsfedlAnomalyDetection.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_params_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params()\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m     86\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpytorch_params_)\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     89\u001b[0m     pl\u001b[38;5;241m.\u001b[39mTrainer()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, X)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/optim/adam.py:146\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 146\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    149\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/TSFEDL/models_pytorch.py:65\u001b[0m, in \u001b[0;36mTSFEDL_BaseModule.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     64\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 65\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(y_hat, y)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/TSFEDL/models_pytorch.py:1323\u001b[0m, in \u001b[0;36mLihOhShu.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m-> 1323\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolutions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m     x \u001b[38;5;241m=\u001b[39m flip_indices_for_conv_to_lstm(x)\n\u001b[1;32m   1325\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 126, 20], expected input[1, 400, 126] to have 126 channels, but got 400 channels instead"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_2 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_2 = torch.utils.data.DataLoader(train_dataset_2, batch_size = 1, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"liohshu\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=10, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2684118/2808341109.py\", line 1, in <module>\n",
      "    scores = model2.decision_function(train_loader_2)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/S-ADL/SADL/time_series/algorithms/tsfedl.py\", line 71, in decision_function\n",
      "Exception: TSFEDLerror decision_function(): y_pred.shape differs from y shape.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "scores = model2.decision_function(train_loader_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: Yibogao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | classifier | mia        | 6.5 K \n",
      "1 | module     | Sequential | 1.4 M \n",
      "2 | loss       | MSELoss    | 0     \n",
      "------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.762     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'YiboGao', 'pytorch_params_': {}, 'in_features': 126, 'loss': <function en_loss at 0x7f3a29d31bc0>, 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "YiboGao(\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (module): Sequential(\n",
      "    (0): RTABlock(\n",
      "      (conv1): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(126, 16, kernel_size=(32,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv2): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(16, 16, kernel_size=(32,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (attention): AttentionBranchYiboGao(\n",
      "        (convBlock1): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(16, 16, kernel_size=(32,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (convBlock2): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(16, 16, kernel_size=(32,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (2): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        )\n",
      "        (convBlock3): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(16, 16, kernel_size=(32,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (finalBlock): Sequential(\n",
      "          (0): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(16, 16, kernel_size=(32,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (1): Conv1d(16, 16, kernel_size=(1,), stride=(1,), padding=same)\n",
      "          (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (conv3): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(16, 16, kernel_size=(32,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): RTABlock(\n",
      "      (conv1): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(16, 32, kernel_size=(16,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv2): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (attention): AttentionBranchYiboGao(\n",
      "        (convBlock1): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (convBlock2): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (2): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        )\n",
      "        (convBlock3): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (finalBlock): Sequential(\n",
      "          (0): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
      "          (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (conv3): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(32, 32, kernel_size=(16,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): RTABlock(\n",
      "      (conv1): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(32, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv2): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (attention): AttentionBranchYiboGao(\n",
      "        (convBlock1): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (convBlock2): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (2): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        )\n",
      "        (convBlock3): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (finalBlock): Sequential(\n",
      "          (0): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,), padding=same)\n",
      "          (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (conv3): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): RTABlock(\n",
      "      (conv1): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv2): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (attention): AttentionBranchYiboGao(\n",
      "        (convBlock1): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (convBlock2): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (2): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        )\n",
      "        (convBlock3): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (finalBlock): Sequential(\n",
      "          (0): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,), padding=same)\n",
      "          (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (conv3): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Dropout(p=0.6, inplace=False)\n",
      "    (9): RTABlock(\n",
      "      (conv1): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv2): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (attention): AttentionBranchYiboGao(\n",
      "        (convBlock1): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (convBlock2): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (2): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        )\n",
      "        (convBlock3): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (finalBlock): Sequential(\n",
      "          (0): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), padding=same)\n",
      "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (conv3): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): RTABlock(\n",
      "      (conv1): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv2): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (attention): AttentionBranchYiboGao(\n",
      "        (convBlock1): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (convBlock2): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (2): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        )\n",
      "        (convBlock3): ConvBlockYiboGao(\n",
      "          (module): Sequential(\n",
      "            (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (finalBlock): Sequential(\n",
      "          (0): ConvBlockYiboGao(\n",
      "            (module): Sequential(\n",
      "              (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), padding=same)\n",
      "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (conv3): ConvBlockYiboGao(\n",
      "        (module): Sequential(\n",
      "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (13): Dropout(p=0.6, inplace=False)\n",
      "  )\n",
      "  (loss): MSELoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ec58caa91f4dcdb1f6e583ac1c1e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                           | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f3a2acda310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import torch\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeseriesDataset\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "train_dataset_3 = TimeSeriesDatasetV2(torch.from_numpy(train_data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_3 = torch.utils.data.DataLoader(train_dataset_3, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"yibogao\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=1, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model3 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model3.model = model3.model.double()\n",
    "print(model3.model)\n",
    "model3.fit(train_loader_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model3.decision_function(train_loader_3)\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.56596286, 4.56822094, 4.57054428, 4.57313846, 4.57555748])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo YaoQihang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 10:39:03.393743: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-01 10:39:03.393869: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-01 10:39:03.395567: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-01 10:39:03.420608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-01 10:39:08.459160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'YaoQihang', 'pytorch_params_': {}, 'in_features': 126, 'loss': None, 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "YaoQihang(\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=32, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(126, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Sequential(\n",
      "      (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (5): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Sequential(\n",
      "      (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (9): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (13): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (17): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 32, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (loss): MSELoss()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | classifier   | mia        | 8.1 K \n",
      "1 | convolutions | Sequential | 1.8 M \n",
      "2 | lstm         | LSTM       | 45.6 K\n",
      "3 | loss         | MSELoss    | 0     \n",
      "--------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.372     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6af8bc7bb144f28568be0c22d6b35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                           | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f85cc660cd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_4 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_4 = torch.utils.data.DataLoader(train_dataset_4, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"yaoqihang\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=32, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "print(32 * train_loader_4.__len__())\n",
    "print(len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL: HtetMyetLynn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'HtetMyetLynn', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None, 'use_rnn': None}\n",
      "HtetMyetLynn(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=80, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convLayers): Sequential(\n",
      "    (0): Conv1d(126, 30, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv1d(30, 30, kernel_size=(2,), stride=(1,), padding=same)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(30, 60, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(60, 60, kernel_size=(2,), stride=(1,), padding=same)\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rnn): GRU(60, 40, batch_first=True, dropout=0.2, bidirectional=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | loss       | MSELoss    | 0     \n",
      "1 | classifier | mia        | 10.5 K\n",
      "2 | convLayers | Sequential | 37.1 K\n",
      "3 | rnn        | GRU        | 24.5 K\n",
      "------------------------------------------\n",
      "72.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 K    Total params\n",
      "0.288     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a767950cb4014c4cb9064e8610c0f1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                           | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1040.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f85c34eefd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"htetmyetlynn\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=80, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: YildirimOzal - pendiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'YildirimOzal', 'pytorch_params_': {}, 'input_shape': None, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None, 'train_autoencoder': True}\n",
      "YildirimOzal(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=32, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(126, 16, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (8): ReLU()\n",
      "    (9): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (10): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (3): ReLU()\n",
      "    (4): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (5): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (6): ReLU()\n",
      "    (7): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (8): Conv1d(64, 16, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (9): ReLU()\n",
      "    (10): Flatten(start_dim=1, end_dim=-1)\n",
      "    (11): Linear(in_features=1600, out_features=200, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(1, 32, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | loss       | MSELoss    | 0     \n",
      "1 | classifier | mia        | 8.1 K \n",
      "2 | encoder    | Sequential | 21.7 K\n",
      "3 | decoder    | Sequential | 335 K \n",
      "4 | lstm       | LSTM       | 4.5 K \n",
      "------------------------------------------\n",
      "370 K     Trainable params\n",
      "0         Non-trainable params\n",
      "370 K     Total params\n",
      "1.480     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61a8803957c4d3ab5758bc2245ba14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                           | 0/? [0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 126, 200])) that is different to the input size (torch.Size([32, 1, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8, 126, 200])) that is different to the input size (torch.Size([8, 1, 200])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f5698191590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=126, forecast_size=1, permute_size = (1,0))\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"yildirimozal\", \"input_shape\":(126,126), \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=32, out_features=126), \"max_epochs\": 1, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 200)\n",
      "(32, 126)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "TSFEDLerror decision_function(): y_pred.shape differs from y shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/S-ADL/SADL/time_series/algorithms/tsfedl.py:135\u001b[0m, in \u001b[0;36mTsfedlAnomalyDetection.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTSFEDLerror decision_function(): y_pred.shape differs from y shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     sum_of_norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(y_pred \u001b[38;5;241m-\u001b[39m y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: TSFEDLerror decision_function(): y_pred.shape differs from y shape."
     ]
    }
   ],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL: CaiWenjuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 9.8 K \n",
      "2 | conv1        | Conv1d     | 1.0 K \n",
      "3 | conv2        | Conv1d     | 6.1 K \n",
      "4 | conv3        | Conv1d     | 15.1 K\n",
      "5 | dense_module | Sequential | 63.9 K\n",
      "--------------------------------------------\n",
      "95.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "95.9 K    Total params\n",
      "0.384     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'CaiWenjuan', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'reduction_ratio': None, 'top_module': None}\n",
      "CaiWenjuan(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=67, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (conv1): Conv1d(126, 8, kernel_size=(1,), stride=(1,), padding=same)\n",
      "  (conv2): Conv1d(126, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (conv3): Conv1d(126, 24, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (dense_module): Sequential(\n",
      "    (0): SqueezeAndExcitationModule(\n",
      "      (fully_connected): Sequential(\n",
      "        (0): Linear(in_features=48, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=48, bias=True)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (1): DenseNetDenseBlock(\n",
      "      (module): ModuleList(\n",
      "        (0): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(48, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(48, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(54, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(54, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SqueezeAndExcitationModule(\n",
      "      (fully_connected): Sequential(\n",
      "        (0): Linear(in_features=60, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=60, bias=True)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (3): DenseNetTransitionBlock(\n",
      "      (module): Sequential(\n",
      "        (0): BatchNorm1d(60, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv1d(60, 36, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "      )\n",
      "    )\n",
      "    (4): SqueezeAndExcitationModule(\n",
      "      (fully_connected): Sequential(\n",
      "        (0): Linear(in_features=36, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=36, bias=True)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (5): DenseNetDenseBlock(\n",
      "      (module): ModuleList(\n",
      "        (0): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(36, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(36, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(42, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(42, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(48, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(48, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (3): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(54, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(54, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): SqueezeAndExcitationModule(\n",
      "      (fully_connected): Sequential(\n",
      "        (0): Linear(in_features=60, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=60, bias=True)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (7): DenseNetTransitionBlock(\n",
      "      (module): Sequential(\n",
      "        (0): BatchNorm1d(60, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv1d(60, 36, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "      )\n",
      "    )\n",
      "    (8): SqueezeAndExcitationModule(\n",
      "      (fully_connected): Sequential(\n",
      "        (0): Linear(in_features=36, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=36, bias=True)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (9): DenseNetDenseBlock(\n",
      "      (module): ModuleList(\n",
      "        (0): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(36, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(36, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(42, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(42, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(48, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(48, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (3): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(54, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(54, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (4): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(60, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(60, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (5): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(66, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(66, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): SqueezeAndExcitationModule(\n",
      "      (fully_connected): Sequential(\n",
      "        (0): Linear(in_features=72, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=72, bias=True)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (11): DenseNetTransitionBlock(\n",
      "      (module): Sequential(\n",
      "        (0): BatchNorm1d(72, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv1d(72, 43, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "      )\n",
      "    )\n",
      "    (12): SqueezeAndExcitationModule(\n",
      "      (fully_connected): Sequential(\n",
      "        (0): Linear(in_features=43, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=43, bias=True)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (13): DenseNetDenseBlock(\n",
      "      (module): ModuleList(\n",
      "        (0): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(43, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(43, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(49, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(49, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (2): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(55, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(55, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (3): DenseNetConvBlock(\n",
      "          (module): Sequential(\n",
      "            (0): BatchNorm1d(61, eps=1.001e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv1d(61, 24, kernel_size=(1,), stride=(1,), bias=False)\n",
      "            (3): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (4): ReLU()\n",
      "            (5): Conv1d(24, 6, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): SqueezeAndExcitationModule(\n",
      "      (fully_connected): Sequential(\n",
      "        (0): Linear(in_features=67, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=67, bias=True)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b158ad536541c18616f73f6906cdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f34241c8990>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"caiwenjuan\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=67, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n"
     ]
    }
   ],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: ZhangJin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type       | Params\n",
      "--------------------------------------------------\n",
      "0 | loss               | MSELoss    | 0     \n",
      "1 | classifier         | mia        | 7.7 K \n",
      "2 | convolutions       | ModuleList | 1.8 M \n",
      "3 | spatial_attention  | ModuleList | 218 K \n",
      "4 | temporal_attention | ModuleList | 75    \n",
      "5 | gru                | GRU        | 19.4 K\n",
      "--------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "8.141     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'ZhangJin', 'pytorch_params_': {}, 'decrease_ratio': 2, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "ZhangJin(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=24, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(126, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (3-4): 2 x Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (spatial_attention): ModuleList(\n",
      "    (0): SpatialAttentionBlockZhangJin(\n",
      "      (shared_dense): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): SpatialAttentionBlockZhangJin(\n",
      "      (shared_dense): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (2-4): 3 x SpatialAttentionBlockZhangJin(\n",
      "      (shared_dense): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (temporal_attention): ModuleList(\n",
      "    (0-4): 5 x TemporalAttentionBlockZhangJin(\n",
      "      (conv1): Conv1d(2, 1, kernel_size=(7,), stride=(1,), padding=same)\n",
      "    )\n",
      "  )\n",
      "  (gru): GRU(256, 12, batch_first=True, dropout=0.2, bidirectional=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de99d68a7ec41bf8aa6aad0ec28adcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f34267ff050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"zhangjin\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=24, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n",
      "(32, 126)\n"
     ]
    }
   ],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: KongZhengmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 09:28:08.459166: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 09:28:08.459205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 09:28:08.460458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 09:28:08.467173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 09:28:09.587839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'KongZhengmin', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "KongZhengmin(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=64, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolution): Sequential(\n",
      "    (0): Conv1d(126, 32, kernel_size=(5,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(32, 64, num_layers=2, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | loss        | MSELoss    | 0     \n",
      "1 | classifier  | mia        | 9.7 K \n",
      "2 | convolution | Sequential | 20.2 K\n",
      "3 | lstm        | LSTM       | 58.4 K\n",
      "-------------------------------------------\n",
      "88.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "88.2 K    Total params\n",
      "0.353     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151316c5894b41d3824966463c33ea0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                         | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f2d8583a310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"kongzhengmin\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=64, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: WeiXiaoyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type        | Params\n",
      "---------------------------------------------\n",
      "0 | loss         | MSELoss     | 0     \n",
      "1 | classifier   | mia         | 32.1 K\n",
      "2 | convolutions | Sequential  | 545 K \n",
      "3 | lstm1        | LSTM        | 2.1 M \n",
      "4 | batchNorm    | BatchNorm1d | 1.0 K \n",
      "5 | lstm2        | LSTM        | 2.1 M \n",
      "---------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.124    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'WeiXiaoyan', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "WeiXiaoyan(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=512, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 32, kernel_size=(5,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (lstm1): LSTM(512, 512, batch_first=True)\n",
      "  (batchNorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm2): LSTM(512, 512, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae556d5243e432387a5f2b94dee9c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                         | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f2e7f7e7750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"weixiaoyan\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=512, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: GaoJunLi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 09:34:11.570669: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 09:34:11.570712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 09:34:11.571969: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 09:34:11.578702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 09:34:12.760920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'GaoJunLi', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': <class 'torch.optim.adam.Adam'>, 'optimizer': None, 'top_module': None}\n",
      "GaoJunLi(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=64, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(126, 64, dropout=0.3)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type    | Params\n",
      "---------------------------------------\n",
      "0 | loss       | MSELoss | 0     \n",
      "1 | classifier | mia     | 9.7 K \n",
      "2 | lstm       | LSTM    | 49.2 K\n",
      "---------------------------------------\n",
      "58.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "58.8 K    Total params\n",
      "0.235     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d54436d4774c97a957b8a8f1d43f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f07ecb25e90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"gaojunli\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=64, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: KhanZulfiqar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 09:59:47.826808: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 09:59:47.826850: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 09:59:47.828112: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 09:59:47.835200: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 09:59:48.993998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'KhanZulfiqar', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "KhanZulfiqar(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=10, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 3, kernel_size=(20,), stride=(1,), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(3, 6, kernel_size=(10,), stride=(1,), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(6, 6, kernel_size=(5,), stride=(1,), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv1d(6, 6, kernel_size=(5,), stride=(1,), bias=False)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv1d(6, 6, kernel_size=(10,), stride=(1,), bias=False)\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(6, 10, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 7.0 K \n",
      "2 | convolutions | Sequential | 8.5 K \n",
      "3 | lstm         | LSTM       | 720   \n",
      "--------------------------------------------\n",
      "16.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.2 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4458136f14874b44a5e38ab8b65505b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7fb6cc7a1dd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"khanzulfiqar\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=10, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: ZhengZhenyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 10:08:17.355424: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 10:08:17.355472: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 10:08:17.356738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 10:08:17.363769: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 10:08:18.532052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'ZhengZhenyu', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "ZhengZhenyu(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=256, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (10): ELU(alpha=1.0)\n",
      "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (13): ELU(alpha=1.0)\n",
      "    (14): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (16): ELU(alpha=1.0)\n",
      "    (17): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 256, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 19.3 K\n",
      "2 | convolutions | Sequential | 406 K \n",
      "3 | lstm         | LSTM       | 526 K \n",
      "--------------------------------------------\n",
      "952 K     Trainable params\n",
      "0         Non-trainable params\n",
      "952 K     Total params\n",
      "3.810     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7b8b231ffa4e2ea076f21a070f9786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7fe186cc1c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"zhengzhenyu\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=256, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: WangKejun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'WangKejun', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "WangKejun(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=256, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (10): ELU(alpha=1.0)\n",
      "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (13): ELU(alpha=1.0)\n",
      "    (14): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (16): ELU(alpha=1.0)\n",
      "    (17): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 256, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 19.3 K\n",
      "2 | convolutions | Sequential | 406 K \n",
      "3 | lstm         | LSTM       | 526 K \n",
      "--------------------------------------------\n",
      "952 K     Trainable params\n",
      "0         Non-trainable params\n",
      "952 K     Total params\n",
      "3.810     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99e7c7fab1f4e559c4ea2a3b7994b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f2d64802950>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"wangkejun\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=256, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: ChenChen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 9.7 K \n",
      "2 | convolutions | Sequential | 766 K \n",
      "3 | lstm1        | LSTM       | 6.1 K \n",
      "4 | lstm2        | LSTM       | 25.1 K\n",
      "--------------------------------------------\n",
      "807 K     Trainable params\n",
      "0         Non-trainable params\n",
      "807 K     Total params\n",
      "3.229     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'ChenChen', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "ChenChen(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=64, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 251, kernel_size=(5,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(251, 150, kernel_size=(5,), stride=(1,), padding=valid)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(150, 100, kernel_size=(10,), stride=(1,), padding=valid)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv1d(100, 81, kernel_size=(20,), stride=(1,), padding=valid)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv1d(81, 61, kernel_size=(20,), stride=(1,), padding=valid)\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv1d(61, 14, kernel_size=(10,), stride=(1,), padding=valid)\n",
      "    (16): ReLU()\n",
      "    (17): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm1): LSTM(14, 32, batch_first=True)\n",
      "  (lstm2): LSTM(32, 64, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81fa791a3e94ea8a3b033f4fa5522e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8, 1, 126])) that is different to the input size (torch.Size([8, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f14a5c92350>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=1000, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"chenchen\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=64, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: KimTaeYoung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'KimTaeYoung', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "KimTaeYoung(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=64, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 64, kernel_size=(2,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(64, 64, kernel_size=(2,), stride=(1,), padding=valid)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(64, 64, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 9.7 K \n",
      "2 | convolutions | Sequential | 24.4 K\n",
      "3 | lstm         | LSTM       | 33.3 K\n",
      "--------------------------------------------\n",
      "67.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.4 K    Total params\n",
      "0.270     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469bcdf1584e401fbe9ec13027a5abdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8, 1, 126])) that is different to the input size (torch.Size([8, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f4500ed6e10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=1000, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"kimtaeyoung\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=64, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: GenMinxing pendiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type    | Params\n",
      "---------------------------------------\n",
      "0 | loss       | MSELoss | 0     \n",
      "1 | classifier | mia     | 10.5 K\n",
      "2 | lstm       | LSTM    | 53.8 K\n",
      "---------------------------------------\n",
      "64.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "64.2 K    Total params\n",
      "0.257     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'GenMinxing', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "GenMinxing(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=80, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(126, 40, batch_first=True, bidirectional=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbb17e00a3b4977a66c4d9aaa334baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([12, 1, 126])) that is different to the input size (torch.Size([12, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f7ff7896a50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\"\"\"\n",
    "data, attack_types, classes = dataset.readKDDCup99Dataset()\n",
    "data = sklearn.preprocessing.StandardScaler().fit_transform(data)\n",
    "\n",
    "attack_types_dict = {}\n",
    "cont=0\n",
    "for att in attack_types:\n",
    "    attack_types_dict[att]=cont\n",
    "    cont+=1\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    classes[i] = attack_types_dict[classes[i]]\n",
    "\n",
    "classes[classes!=attack_types_dict[\"normal\"]] = 1\n",
    "classes[classes==attack_types_dict[\"normal\"]] = 0\n",
    "classes_test = classes[-500000:]\n",
    "\n",
    "normal_train = np.where(classes[:-500000]==0)[0][:10000]\n",
    "\"\"\"\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(train_data).double(), window_size=100, forecast_size=1, permute_size = (0,1))\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = True)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"genminxing\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=80, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TimeSeriesDatasetV2.__init__() got an unexpected keyword argument 'permute_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset_5 \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesDatasetV2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnormal_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermute_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_loader_5 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset_5, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm_\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenminxing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_module\u001b[39m\u001b[38;5;124m\"\u001b[39m: mia(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m126\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_features\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m126\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_parser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n",
      "\u001b[0;31mTypeError\u001b[0m: TimeSeriesDatasetV2.__init__() got an unexpected keyword argument 'permute_size'"
     ]
    }
   ],
   "source": [
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data[normal_train]).double(), window_size=100, forecast_size=1, permute_size = (1,0))\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = True)\n",
    "kwargs = {\"algorithm_\": \"genminxing\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=80, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: FuJiangmeng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 11:07:23.652206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 11:07:23.652250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 11:07:23.653497: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 11:07:23.660724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 11:07:24.720684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'FuJiangmeng', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "FuJiangmeng(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=256, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
      "    (1): Tanh()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(32, 256, batch_first=True, dropout=0.3)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 19.3 K\n",
      "2 | convolutions | Sequential | 4.1 K \n",
      "3 | lstm         | LSTM       | 296 K \n",
      "--------------------------------------------\n",
      "320 K     Trainable params\n",
      "0         Non-trainable params\n",
      "320 K     Total params\n",
      "1.281     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fc1da677f7472da15ee722b64c0662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f76baeab850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"fujiangmeng\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=256, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: ShiHaotian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | loss          | MSELoss    | 0     \n",
      "1 | classifier    | mia        | 8.1 K \n",
      "2 | convolutions1 | Sequential | 52.4 K\n",
      "3 | convolutions2 | Sequential | 52.4 K\n",
      "4 | convolutions3 | Sequential | 52.4 K\n",
      "5 | lstm          | LSTM       | 16.6 K\n",
      "---------------------------------------------\n",
      "182 K     Trainable params\n",
      "0         Non-trainable params\n",
      "182 K     Total params\n",
      "0.728     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'ShiHaotian', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "ShiHaotian(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=32, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions1): Sequential(\n",
      "    (0): Conv1d(126, 32, kernel_size=(13,), stride=(2,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutions2): Sequential(\n",
      "    (0): Conv1d(126, 32, kernel_size=(13,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convolutions3): Sequential(\n",
      "    (0): Conv1d(126, 32, kernel_size=(13,), stride=(2,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(96, 32, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb36086ea6241498adfb75a5eaf81ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7fb1741f2810>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"shihaotian\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=32, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: HuangMeiLing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 7.4 K \n",
      "2 | convolutions | Sequential | 177 K \n",
      "--------------------------------------------\n",
      "184 K     Trainable params\n",
      "0         Non-trainable params\n",
      "184 K     Total params\n",
      "0.738     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'HuangMeiLing', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "HuangMeiLing(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=19, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): ConstantPad1d(padding=(1, 1), value=0)\n",
      "    (1): Conv1d(126, 48, kernel_size=(15,), stride=(6,), padding=valid)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): ConstantPad1d(padding=(3, 3), value=0)\n",
      "    (5): Conv1d(48, 256, kernel_size=(7,), stride=(2,), padding=valid)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04f4c538c444781a6108e2623a6db99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                           | 0/? [0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([20, 1, 126])) that is different to the input size (torch.Size([20, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7ff5f99b8590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=252, forecast_size=1, permute_size=(1,0))\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"huangmeiling\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=19, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: HongTan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 11:38:01.408173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 11:38:01.408205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 11:38:01.409427: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 11:38:01.416723: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 11:38:02.602529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'HongTan', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "HongTan(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=4, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 40, kernel_size=(5,), stride=(1,), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(40, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm1): LSTM(32, 32, batch_first=True)\n",
      "  (lstm2): LSTM(32, 16, batch_first=True)\n",
      "  (lstm3): LSTM(16, 4, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 6.7 K \n",
      "2 | convolutions | Sequential | 29.0 K\n",
      "3 | lstm1        | LSTM       | 8.4 K \n",
      "4 | lstm2        | LSTM       | 3.2 K \n",
      "5 | lstm3        | LSTM       | 352   \n",
      "--------------------------------------------\n",
      "47.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "47.7 K    Total params\n",
      "0.191     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c947998f7d4c94928e86b66cad81a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                           | 0/? [0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f4fb56bb450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1, permute_size=(1,0))\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"hongtan\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=4, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: SharPar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | loss         | MSELoss    | 0     \n",
      "1 | classifier   | mia        | 7.3 K \n",
      "2 | convolutions | Sequential | 40.4 K\n",
      "3 | lstm1        | LSTM       | 12.5 K\n",
      "4 | lstm2        | LSTM       | 3.2 K \n",
      "5 | fcc_module   | Sequential | 544   \n",
      "--------------------------------------------\n",
      "63.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "63.9 K    Total params\n",
      "0.256     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'SharPar', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "SharPar(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=16, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(126, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (lstm1): LSTM(64, 32, batch_first=True)\n",
      "  (lstm2): LSTM(32, 16, batch_first=True)\n",
      "  (fcc_module): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1d056b56fd4a0d9798bc050903341a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 126])) that is different to the input size (torch.Size([32, 126])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f6995b58210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=400, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"sharpar\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=16, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: DaiXiLi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name       | Type      | Params\n",
      "------------------------------------------\n",
      "0  | loss       | MSELoss   | 0     \n",
      "1  | classifier | mia       | 108 K \n",
      "2  | v2c1       | Conv1d    | 10.3 K\n",
      "3  | v2mp1      | MaxPool1d | 0     \n",
      "4  | v2d1       | Linear    | 2.0 K \n",
      "5  | v3c1       | Conv1d    | 80.8 K\n",
      "6  | v3mp1      | MaxPool1d | 0     \n",
      "7  | v3c2       | Conv1d    | 41.0 K\n",
      "8  | v3mp2      | MaxPool1d | 0     \n",
      "9  | v3c3       | Conv1d    | 10.3 K\n",
      "10 | v3mp3      | MaxPool1d | 0     \n",
      "11 | v3d1       | Linear    | 2.0 K \n",
      "12 | v1c1       | Conv1d    | 41.0 K\n",
      "13 | v1mp1      | MaxPool1d | 0     \n",
      "14 | v1d1       | Linear    | 4.0 K \n",
      "------------------------------------------\n",
      "300 K     Trainable params\n",
      "0         Non-trainable params\n",
      "300 K     Total params\n",
      "1.201     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': True, 'algorithm_': 'DaiXiLi', 'pytorch_params_': {}, 'in_features': 126, 'loss': CrossEntropyLoss(), 'metrics': None, 'optimizer': None, 'top_module': None}\n",
      "DaiXiLi(\n",
      "  (loss): MSELoss()\n",
      "  (classifier): mia(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=2048, out_features=50, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=50, out_features=126, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (v2c1): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (v2mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (v2d1): Linear(in_features=125, out_features=16, bias=True)\n",
      "  (v3c1): Conv1d(126, 128, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (v3mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (v3c2): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (v3mp2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (v3c3): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (v3mp3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (v3d1): Linear(in_features=125, out_features=16, bias=True)\n",
      "  (v1c1): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (v1mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (v1d1): Linear(in_features=250, out_features=16, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224c1ee4aa0a4de1865ee3ff195e5681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSFEDLerror fit():  mat1 and mat2 shapes cannot be multiplied (2048x100 and 250x16)\n",
      "For further reference please see: https://s-tsfe-dl.readthedocs.io/en/latest/index.html\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2048x100 and 250x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m model2\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(model2\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/S-ADL/SADL/time_series/algorithms/tsfedl.py:87\u001b[0m, in \u001b[0;36mTsfedlAnomalyDetection.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_params_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params()\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m     86\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpytorch_params_)\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     89\u001b[0m     pl\u001b[38;5;241m.\u001b[39mTrainer()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, X)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/optim/adam.py:146\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 146\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    149\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/TSFEDL/models_pytorch.py:65\u001b[0m, in \u001b[0;36mTSFEDL_BaseModule.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     64\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 65\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(y_hat, y)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/TSFEDL/models_pytorch.py:2688\u001b[0m, in \u001b[0;36mDaiXiLi.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2686\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv1c1(x3)\n\u001b[1;32m   2687\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv1mp1(x1)\n\u001b[0;32m-> 2688\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1d1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2690\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv3c2(x3)\n\u001b[1;32m   2691\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv3mp2(x3)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x100 and 250x16)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/homeGPU/mbautista/S-ADL\")\n",
    "import SADL.time_series.time_series_datasets as dataset\n",
    "from SADL.time_series.time_series_utils import TimeSeriesDatasetV2\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "\n",
    "class mia(torch.nn.Module):\n",
    "    def __init__(self, in_features=103, out_features=103, npred=1):\n",
    "        super(mia, self).__init__()\n",
    "        self.npred = npred\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(in_features=in_features, out_features=50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=50, out_features=npred*out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if len(out.shape)>2:\n",
    "            out = out[:, -1, :]\n",
    "        if self.npred > 1:\n",
    "            # Reshape to (batch_size, npred, out_features)\n",
    "            out = out.reshape(out.shape[0], self.npred, -1)\n",
    "        return out   \n",
    "\n",
    "\n",
    "data = train_data\n",
    "train_dataset_5 = TimeSeriesDatasetV2(torch.from_numpy(data).double(), window_size=1000, forecast_size=1)\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_dataset_5, batch_size = 32, shuffle = False)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"daixili\", \"loss\": torch.nn.MSELoss(),\"top_module\": mia(in_features=2048, out_features=126), \"max_epochs\": 1, \"in_features\":126, \"label_parser\": True}\n",
    "model2 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model2.model = model2.model.double()\n",
    "print(model2.model)\n",
    "model2.fit(train_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2048x100 and 250x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/S-ADL/SADL/time_series/algorithms/tsfedl.py:99\u001b[0m, in \u001b[0;36mTsfedlAnomalyDetection.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     97\u001b[0m decision_scores_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, y \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[0;32m---> 99\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    102\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/TSFEDL/models_pytorch.py:2688\u001b[0m, in \u001b[0;36mDaiXiLi.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2686\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv1c1(x3)\n\u001b[1;32m   2687\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv1mp1(x1)\n\u001b[0;32m-> 2688\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1d1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2690\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv3c2(x3)\n\u001b[1;32m   2691\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv3mp2(x3)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x100 and 250x16)"
     ]
    }
   ],
   "source": [
    "scores = model2.decision_function(train_loader_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
