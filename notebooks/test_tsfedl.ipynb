{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "done.\n",
      "X shape:  (16499, 1000, 1)\n",
      "y shape:  (16499,)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def get_segments(data: wfdb.Record,\n",
    "                 annotations: wfdb.Annotation,\n",
    "                 labels: np.ndarray,\n",
    "                 left_offset: int = 99,\n",
    "                 right_offset: int = 160,\n",
    "                 fixed_length: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" It generates the segments of uninterrupted sequences of arrythmia beats into the corresponding arrythmia groups\n",
    "    in labels.\n",
    "\n",
    "    :param data:            The arrythmia signal as a wfdb Record class\n",
    "    :param annotations:     The set of annotations as a wfdb Annotation class\n",
    "    :param labels:          The set of valid labels for the different segments. Segments with different labels are discarded\n",
    "    :param left_offset:     The number of instance at the left of the first R peak of the segment. Default to 99\n",
    "    :param right_offset:    The number of instances at the right of the last R peak of the segment. Default to 160\n",
    "    :param fixed_length:    Should the segments have a fixed length? If fixed_length is a number, then the segments will\n",
    "                            have the specified length. If the segment length is greater than fixed_length, it is truncated\n",
    "                            or padded with zeros otherwise. Default to None.\n",
    "\n",
    "    :return:                A tuple that contains the data and the associated labels. Data has a shape of (N, T, V)\n",
    "                            where N is the number of segments (or instances), V is the number of variables (1 in this case)\n",
    "                            and T is the number of timesteps of each segment.  Labels are numerically encoded according to the\n",
    "                            value passed in the :parameter labels param.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    annot_segments = []\n",
    "\n",
    "    # Get the tuples for consecutive symbols. The tuple is (first, last, symbol) where first is the index of the first occurrence of symbol,\n",
    "    # and last is the index of the last consecutive ocurrence.\n",
    "    while(i < len(annotations.symbol)):\n",
    "        first = i\n",
    "        current_symbol = annotations.symbol[i]\n",
    "        while(i < len(annotations.symbol) and annotations.symbol[i] == current_symbol):\n",
    "            i += 1\n",
    "        last = i-1\n",
    "        tup = (first, last, current_symbol)\n",
    "        annot_segments.append(tup)\n",
    "\n",
    "    # Now, for each extracted tuple, get the X segments:\n",
    "    result = []\n",
    "    classes = []\n",
    "    for s in annot_segments:  # s is a tuple (first, last, symbol)\n",
    "        if s[2] in labels:\n",
    "            classes.append(s[2])\n",
    "            init = annotations.sample[s[0]] - left_offset\n",
    "            if init < 0:\n",
    "                init = 0\n",
    "\n",
    "            end = annotations.sample[s[1]] + right_offset\n",
    "            if end >= len(data.p_signal):\n",
    "                end = len(data.p_signal) - 1\n",
    "\n",
    "            r = range(init, end)\n",
    "\n",
    "            # Get the samples of the segments (p_signal is a 2D array, we only want the first axis)\n",
    "            new_segment = np.array(data.p_signal[r, 1], dtype='float32')\n",
    "\n",
    "            # truncate or pad with zeros the segment if necessary\n",
    "            if (fixed_length != None):\n",
    "                if (len(new_segment) > fixed_length):  # truncate\n",
    "                    new_segment = new_segment[:fixed_length]\n",
    "                elif (len(new_segment < fixed_length)):  # pad with zeros to the right\n",
    "                    number_of_zeros = fixed_length - len(new_segment)\n",
    "                    new_segment = np.pad(new_segment, (0, number_of_zeros), mode='constant', constant_values=0)\n",
    "\n",
    "            result.append(new_segment)\n",
    "\n",
    "    result = np.stack(result, axis=0)\n",
    "    result = np.reshape(result, (result.shape[0], result.shape[1], 1))  # shape[0] segments with 1 variable, with shape[1] timestamps each\n",
    "    classes = np.array(classes, dtype=str)\n",
    "\n",
    "    # Encode labels: from string to numeric.\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels)\n",
    "    classes = label_encoder.transform(classes)\n",
    "\n",
    "    return (result, classes)\n",
    "\n",
    "\n",
    "\n",
    "def read_MIT_BIH(path: str,\n",
    "                 labels: np.ndarray = np.array(['N','L','R','A','V']),\n",
    "                 left_offset: int = 99,\n",
    "                 right_offset: int = 160,\n",
    "                 fixed_length: Optional[int] = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" It reads the MIT-BIH Arrythmia X with the specified default configuration of the work presented at:\n",
    "    Oh, Shu Lih, et al. \"Automated diagnosis of arrhythmia using combination of CNN and LSTM techniques with\n",
    "    variable length heart beats.\" Computers in biology and medicine 102 (2018): 278-287.\n",
    "\n",
    "    :param labels:              The labels of the different types of arrythmia to be employed\n",
    "    :param path:                The path of the directory where the X files are stored. Note: The X and annotations\n",
    "                                files must have the same name, but different extension (annotations must have .atr extension)\n",
    "    :param left_offset:         The number of instances at the left of the first R peak of the segment. Defaults to 99\n",
    "    :param right_offset:        The number of instances at the right of the last R peak of the segment. Defaults to 160\n",
    "    :param fixed_length:        If different to None, the segment will have the specified number of instances. Note that\n",
    "                                if the segment length > fixed_length it will be truncate or padded with zeros otherwise.\n",
    "\n",
    "    :return:                     A tuple that contains the data and the associated labels as an ndarray. Data has a shape of (N, T, V)\n",
    "                                where N is the number of segments (or instances), V is the number of variables (1 in this case)\n",
    "                                and T is the number of timesteps of each segment.  Labels are numerically encoded according to the\n",
    "                                value passed in the :parameter labels param.\n",
    "    \"\"\"\n",
    "    print(\"reading data...\")\n",
    "    segments = []\n",
    "    classes = []\n",
    "\n",
    "    files = [ file[:-4] for file in os.listdir(path) if file.endswith('.dat') ]\n",
    "    for f in files:\n",
    "        data = wfdb.rdrecord(path + f)\n",
    "        annotation = wfdb.rdann(path + f, 'atr')\n",
    "\n",
    "        s, clazz = get_segments(data=data,\n",
    "                                 annotations=annotation,\n",
    "                                 labels=labels,\n",
    "                                 left_offset=left_offset,\n",
    "                                 right_offset=right_offset,\n",
    "                                 fixed_length=fixed_length)\n",
    "\n",
    "        segments.append(s)\n",
    "        classes.append(clazz)\n",
    "\n",
    "    segments = np.vstack(segments)\n",
    "    classes = np.concatenate(classes)\n",
    "    print(\"done.\")\n",
    "\n",
    "    return (segments, classes)\n",
    "\n",
    "# Leemos los datos\n",
    "dir = \"../physionet.org/files/mitdb/1.0.0/\"\n",
    "\n",
    "X, y = read_MIT_BIH(dir)\n",
    "\n",
    "# mostramos la forma de los datos de entrada. En total tenemos 16499 series temporales \n",
    "# de 1 variable con 1000 instantes de tiempo cada una de ellas.\n",
    "# Cada serie temporal tiene Ãºnicamente 1 valor asociado o clase que determina el tipo de arritmia\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "reading data...\n",
      "done.\n",
      "OhShuLih(\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (classifier): OhShuLih_Classifier(\n",
      "    (model): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=20, out_features=20, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=20, out_features=10, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=10, out_features=5, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (convolutions): Sequential(\n",
      "    (0): Conv1d(1, 3, kernel_size=(20,), stride=(1,), padding=(19,), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(3, 6, kernel_size=(10,), stride=(1,), padding=(9,), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(6, 6, kernel_size=(5,), stride=(1,), padding=(4,), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(6, 20, batch_first=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/homeGPU/mbautista/s-adl-environment/lib/python3 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                | Params\n",
      "-----------------------------------------------------\n",
      "0 | loss         | CrossEntropyLoss    | 0     \n",
      "1 | classifier   | OhShuLih_Classifier | 685   \n",
      "2 | convolutions | Sequential          | 420   \n",
      "3 | lstm         | LSTM                | 2.2 K \n",
      "-----------------------------------------------------\n",
      "3.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n",
      "/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b62c6be8614c868df8616255f8dd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |              | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SADL.time_series.algorithms.tsfedl.TsfedlAnomalyDetection at 0x7f8ac0b01410>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# temporary solution for relative imports in case sadl is not installed\n",
    "# if sadl is installed, no need to use the line\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from TSFEDL.data import MIT_BIH\n",
    "from inspect import signature\n",
    "from inspect import signature\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "from TSFEDL.models_pytorch import OhShuLih\n",
    "\n",
    "mit_bih = MIT_BIH(path=\"../physionet.org/files/mitdb/1.0.0/\", return_hot_coded=False)\n",
    "mit_bih.x = mit_bih.x[:10]\n",
    "mit_bih.y = mit_bih.y[:10]\n",
    "tra_size = int(len(mit_bih) * 0.8)\n",
    "tst_size = len(mit_bih) - tra_size\n",
    "train, test = torch.utils.data.random_split(mit_bih, [tra_size, tst_size])\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=1, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=1, num_workers=0)\n",
    "    \n",
    "#kwargs = {\"algorithm_\": \"ohshulih_classifier\", \"in_features\":1, \"n_classes\":4, \"max_epochs\": 1, \"label_parser\": True}\n",
    "#model1 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"ohshulih\", \"in_features\":1, \"label_parser\": True}\n",
    "model1 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "model = OhShuLih(in_features=1,loss=nn.CrossEntropyLoss(),optimizer=torch.optim.Adam,lr=0.001)\n",
    "\n",
    "print(model1.model)\n",
    "#print(model)\n",
    "model1.fit(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv1d() received an invalid combination of arguments - got (Subset, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Subset!, !Parameter!, !NoneType!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Subset!, !Parameter!, !NoneType!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/S-ADL/SADL/time_series/algorithms/tsfedl.py:52\u001b[0m, in \u001b[0;36mTsfedlAnomalyDetection.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 52\u001b[0m     X_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(X_pred\u001b[38;5;241m-\u001b[39mX), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/TSFEDL/models_pytorch.py:208\u001b[0m, in \u001b[0;36mOhShuLih.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 208\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolutions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# Now, flip indices using a view for the LSTM as it requires a shape of (N, L, H_in = C)\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/homeGPU/mbautista/s-adl-environment/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv1d() received an invalid combination of arguments - got (Subset, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Subset!, !Parameter!, !NoneType!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Subset!, !Parameter!, !NoneType!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, int)\n"
     ]
    }
   ],
   "source": [
    "print(model1.decision_function(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
