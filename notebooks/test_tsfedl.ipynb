{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "done.\n",
      "X shape:  (16499, 1000, 1)\n",
      "y shape:  (16499,)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def get_segments(data: wfdb.Record,\n",
    "                 annotations: wfdb.Annotation,\n",
    "                 labels: np.ndarray,\n",
    "                 left_offset: int = 99,\n",
    "                 right_offset: int = 160,\n",
    "                 fixed_length: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" It generates the segments of uninterrupted sequences of arrythmia beats into the corresponding arrythmia groups\n",
    "    in labels.\n",
    "\n",
    "    :param data:            The arrythmia signal as a wfdb Record class\n",
    "    :param annotations:     The set of annotations as a wfdb Annotation class\n",
    "    :param labels:          The set of valid labels for the different segments. Segments with different labels are discarded\n",
    "    :param left_offset:     The number of instance at the left of the first R peak of the segment. Default to 99\n",
    "    :param right_offset:    The number of instances at the right of the last R peak of the segment. Default to 160\n",
    "    :param fixed_length:    Should the segments have a fixed length? If fixed_length is a number, then the segments will\n",
    "                            have the specified length. If the segment length is greater than fixed_length, it is truncated\n",
    "                            or padded with zeros otherwise. Default to None.\n",
    "\n",
    "    :return:                A tuple that contains the data and the associated labels. Data has a shape of (N, T, V)\n",
    "                            where N is the number of segments (or instances), V is the number of variables (1 in this case)\n",
    "                            and T is the number of timesteps of each segment.  Labels are numerically encoded according to the\n",
    "                            value passed in the :parameter labels param.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    annot_segments = []\n",
    "\n",
    "    # Get the tuples for consecutive symbols. The tuple is (first, last, symbol) where first is the index of the first occurrence of symbol,\n",
    "    # and last is the index of the last consecutive ocurrence.\n",
    "    while(i < len(annotations.symbol)):\n",
    "        first = i\n",
    "        current_symbol = annotations.symbol[i]\n",
    "        while(i < len(annotations.symbol) and annotations.symbol[i] == current_symbol):\n",
    "            i += 1\n",
    "        last = i-1\n",
    "        tup = (first, last, current_symbol)\n",
    "        annot_segments.append(tup)\n",
    "\n",
    "    # Now, for each extracted tuple, get the X segments:\n",
    "    result = []\n",
    "    classes = []\n",
    "    for s in annot_segments:  # s is a tuple (first, last, symbol)\n",
    "        if s[2] in labels:\n",
    "            classes.append(s[2])\n",
    "            init = annotations.sample[s[0]] - left_offset\n",
    "            if init < 0:\n",
    "                init = 0\n",
    "\n",
    "            end = annotations.sample[s[1]] + right_offset\n",
    "            if end >= len(data.p_signal):\n",
    "                end = len(data.p_signal) - 1\n",
    "\n",
    "            r = range(init, end)\n",
    "\n",
    "            # Get the samples of the segments (p_signal is a 2D array, we only want the first axis)\n",
    "            new_segment = np.array(data.p_signal[r, 1], dtype='float32')\n",
    "\n",
    "            # truncate or pad with zeros the segment if necessary\n",
    "            if (fixed_length != None):\n",
    "                if (len(new_segment) > fixed_length):  # truncate\n",
    "                    new_segment = new_segment[:fixed_length]\n",
    "                elif (len(new_segment < fixed_length)):  # pad with zeros to the right\n",
    "                    number_of_zeros = fixed_length - len(new_segment)\n",
    "                    new_segment = np.pad(new_segment, (0, number_of_zeros), mode='constant', constant_values=0)\n",
    "\n",
    "            result.append(new_segment)\n",
    "\n",
    "    result = np.stack(result, axis=0)\n",
    "    result = np.reshape(result, (result.shape[0], result.shape[1], 1))  # shape[0] segments with 1 variable, with shape[1] timestamps each\n",
    "    classes = np.array(classes, dtype=str)\n",
    "\n",
    "    # Encode labels: from string to numeric.\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels)\n",
    "    classes = label_encoder.transform(classes)\n",
    "\n",
    "    return (result, classes)\n",
    "\n",
    "\n",
    "\n",
    "def read_MIT_BIH(path: str,\n",
    "                 labels: np.ndarray = np.array(['N','L','R','A','V']),\n",
    "                 left_offset: int = 99,\n",
    "                 right_offset: int = 160,\n",
    "                 fixed_length: Optional[int] = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" It reads the MIT-BIH Arrythmia X with the specified default configuration of the work presented at:\n",
    "    Oh, Shu Lih, et al. \"Automated diagnosis of arrhythmia using combination of CNN and LSTM techniques with\n",
    "    variable length heart beats.\" Computers in biology and medicine 102 (2018): 278-287.\n",
    "\n",
    "    :param labels:              The labels of the different types of arrythmia to be employed\n",
    "    :param path:                The path of the directory where the X files are stored. Note: The X and annotations\n",
    "                                files must have the same name, but different extension (annotations must have .atr extension)\n",
    "    :param left_offset:         The number of instances at the left of the first R peak of the segment. Defaults to 99\n",
    "    :param right_offset:        The number of instances at the right of the last R peak of the segment. Defaults to 160\n",
    "    :param fixed_length:        If different to None, the segment will have the specified number of instances. Note that\n",
    "                                if the segment length > fixed_length it will be truncate or padded with zeros otherwise.\n",
    "\n",
    "    :return:                     A tuple that contains the data and the associated labels as an ndarray. Data has a shape of (N, T, V)\n",
    "                                where N is the number of segments (or instances), V is the number of variables (1 in this case)\n",
    "                                and T is the number of timesteps of each segment.  Labels are numerically encoded according to the\n",
    "                                value passed in the :parameter labels param.\n",
    "    \"\"\"\n",
    "    print(\"reading data...\")\n",
    "    segments = []\n",
    "    classes = []\n",
    "\n",
    "    files = [ file[:-4] for file in os.listdir(path) if file.endswith('.dat') ]\n",
    "    for f in files:\n",
    "        data = wfdb.rdrecord(path + f)\n",
    "        annotation = wfdb.rdann(path + f, 'atr')\n",
    "\n",
    "        s, clazz = get_segments(data=data,\n",
    "                                 annotations=annotation,\n",
    "                                 labels=labels,\n",
    "                                 left_offset=left_offset,\n",
    "                                 right_offset=right_offset,\n",
    "                                 fixed_length=fixed_length)\n",
    "\n",
    "        segments.append(s)\n",
    "        classes.append(clazz)\n",
    "\n",
    "    segments = np.vstack(segments)\n",
    "    classes = np.concatenate(classes)\n",
    "    print(\"done.\")\n",
    "\n",
    "    return (segments, classes)\n",
    "\n",
    "# Leemos los datos\n",
    "dir = \"physionet.org/files/mitdb/1.0.0/\"\n",
    "\n",
    "X, y = read_MIT_BIH(dir)\n",
    "\n",
    "# mostramos la forma de los datos de entrada. En total tenemos 16499 series temporales \n",
    "# de 1 variable con 1000 instantes de tiempo cada una de ellas.\n",
    "# Cada serie temporal tiene únicamente 1 valor asociado o clase que determina el tipo de arritmia\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "reading data...\n",
      "done.\n",
      "aaaain_features\n",
      "aaaaloss\n",
      "aaaametrics\n",
      "aaaaoptimizer\n",
      "aaaatop_module\n",
      "{'label_parser': True, 'algorithm_': 'OhShuLih', 'in_features': 1, 'loss': CrossEntropyLoss(), 'metrics': None, 'pytorch_params_': {'max_epochs': 200}}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from TSFEDL.data import MIT_BIH\n",
    "from inspect import signature\n",
    "from inspect import signature\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "from SADL.time_series.algorithms import tsfedl\n",
    "from TSFEDL.models_pytorch import OhShuLih\n",
    "\n",
    "mit_bih = MIT_BIH(path=\"physionet.org/files/mitdb/1.0.0/\", return_hot_coded=False)\n",
    "mit_bih.x = mit_bih.x[:10]\n",
    "mit_bih.y = mit_bih.y[:10]\n",
    "tra_size = int(len(mit_bih) * 0.8)\n",
    "tst_size = len(mit_bih) - tra_size\n",
    "train, test = torch.utils.data.random_split(mit_bih, [tra_size, tst_size])\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=1, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=1, num_workers=0)\n",
    "    \n",
    "#kwargs = {\"algorithm_\": \"ohshulih_classifier\", \"in_features\":1, \"n_classes\":4, \"max_epochs\": 1, \"label_parser\": True}\n",
    "#model1 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "\n",
    "kwargs = {\"algorithm_\": \"ohshulih\", \"max_epochs\": 200, \"in_features\":1, \"label_parser\": True}\n",
    "model1 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "\n",
    "print(model1.get_params())\n",
    "print(model1.model.top_module)\n",
    "#print(model1.model)\n",
    "#print(model)\n",
    "#model1.fit(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                | Params\n",
      "-----------------------------------------------------\n",
      "0 | loss         | CrossEntropyLoss    | 0     \n",
      "1 | classifier   | OhShuLih_Classifier | 685   \n",
      "2 | convolutions | Sequential          | 420   \n",
      "3 | lstm         | LSTM                | 2.2 K \n",
      "-----------------------------------------------------\n",
      "3.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af68ce05c9b4292a08ba353da5a96d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce1eb45c3384979a9206fbd566965f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch                 1.0\n",
      "     test_loss_epoch       0.0014386551920324564\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "1.0\n",
      "tensor([  3.5918, -13.7475,  -2.3540, -13.3412, -13.1747],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([  3.9990, -15.2864,  -2.5999, -14.8926, -14.6999],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "def acc_from_logits(y_hat, y):\n",
    "    y_hat = F.softmax(y_hat, dim=1)\n",
    "    preds = torch.argmax(y_hat, dim=1)\n",
    "    acc = (preds == y).sum().item() / len(y)\n",
    "    return acc\n",
    "\n",
    "#print(train.shape)\n",
    "kwargs = {\"max_epochs\": 200,\"algorithm_\": \"ohshulih\", \"in_features\": 1, \"loss\": nn.CrossEntropyLoss(), \"metrics\":{\"acc\": acc_from_logits},}\n",
    "model1 = tsfedl.TsfedlAnomalyDetection(**kwargs)\n",
    "#model1.fit(train_loader)\n",
    "trainer = pl.Trainer(max_epochs=200)\n",
    "trainer.fit(model1.model, train_loader)\n",
    "test_results = trainer.test(model1.model, test_loader)\n",
    "print(test_results[0]['test_acc_epoch'])\n",
    "#X_pred = model1.model(X_test)\n",
    "#print(X_pred)\n",
    "for data, labels in test_loader:\n",
    "    X_pred = model1.model(data)\n",
    "    print(X_pred[0])\n",
    "#model1.decision_function(test_loader)\n",
    "\n",
    "#model1.decision_function(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_parser': None, 'algorithm_': 'OhShuLih', 'in_features': 1, 'loss': CrossEntropyLoss(), 'metrics': {'acc': <function acc_from_logits at 0x7fce22e9d300>}, 'pytorch_params_': {'max_epochs': 200}}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"max_epochs\": 200,\"algorithm_\": \"ohshulih\", \"in_features\": 1, \"loss\": nn.CrossEntropyLoss(), \"metrics\":{\"acc\": acc_from_logits},}\n",
    "#model1.set_params(**kwargs)\n",
    "#model1.fit(train_loader)\n",
    "print(model1.get_params())\n",
    "test_results = None\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model1.model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_metrics = {name: 0.0 for name in model1.model.metrics.keys()}\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Unpack batch\n",
    "        x, y = batch\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = model1.model(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model1.model.loss(y_hat, y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Compute metrics\n",
    "        for name, f in model1.model.metrics.items():\n",
    "            value = f(y_hat, y)\n",
    "            test_metrics[name] += value\n",
    "\n",
    "    # Compute average loss and metrics\n",
    "    num_batches = len(test_loader)\n",
    "    test_loss /= num_batches\n",
    "    test_metrics = {name: value / num_batches for name, value in test_metrics.items()}\n",
    "    print(test_metrics['acc'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
